{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mkl\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/1e/c713b011b90cd238023df1c0025130c40bc40870a46273d942e89114233c/mkl-2019.0-py2.py3-none-macosx_10_12_intel.macosx_10_12_x86_64.whl (193.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 193.8MB 256kB/s ta 0:00:011   35% |███████████▏                    | 67.9MB 7.5MB/s eta 0:00:17    90% |████████████████████████████▉   | 174.6MB 5.6MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting intel-openmp (from mkl)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/51/1138f9df9fa1659c035927297d275a57404f174a1405febe4a5084e77320/intel_openmp-2019.0-py2.py3-none-macosx_10_12_intel.macosx_10_12_x86_64.whl (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 3.3MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: intel-openmp, mkl\n",
      "Successfully installed intel-openmp-2019.0 mkl-2019.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.manifold import TSNE\n",
    "# ref: https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN+DailyMail Stories 312085\n"
     ]
    }
   ],
   "source": [
    "# Load CNN\n",
    "import pickle\n",
    "news_summaries = pickle.load(open('/Users/dbm/Documents/Insight S19/data/cnn_dmail_news_summary.pkl', 'rb'))\n",
    "print('Loaded CNN+DailyMail Stories %d' % len(news_summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = []\n",
    "summary = []\n",
    "# range(len(master))\n",
    "for i in news_summaries:\n",
    "#     story[\"story\"] = nltk.wordpunct_tokenize(story[\"story\"])\n",
    "#     story[\"summary\"] = nltk.wordpunct_tokenize(story[\"summary\"])\n",
    "#     story.append(nltk.wordpunct_tokenize(story[\"story\"]))\n",
    "#     summary.append(nltk.wordpunct_tokenize(story[\"story\"]))\n",
    "    story.append(i[\"story\"])\n",
    "    summary.append(i[\"summary\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8444059 unique tokens in the stories.\n"
     ]
    }
   ],
   "source": [
    "# For story\n",
    "tknzr_story = Tokenizer(num_words = 300000)\n",
    "tknzr_story.fit_on_texts(story)\n",
    "seq_story = tknzr_story.texts_to_sequences(story)\n",
    "\n",
    "word_idx_story = tknzr_story.word_index\n",
    "print('Found %s unique tokens in the stories.' % len(word_idx_story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1123526 unique tokens in the summaries.\n"
     ]
    }
   ],
   "source": [
    "# For summary\n",
    "tknzr_summary = Tokenizer(num_words = 300000)\n",
    "tknzr_summary.fit_on_texts(summary)\n",
    "seq_summary = tknzr_summary.texts_to_sequences(summary)\n",
    "\n",
    "word_idx_summary = tknzr_summary.word_index\n",
    "print('Found %s unique tokens in the summaries.' % len(word_idx_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_length = []\n",
    "# story_length = []\n",
    "# for i in news_summaries:\n",
    "#     story_length.append(len(i[\"story\"]))\n",
    "#     summary_length.append(len(i[\"summary\"]))\n",
    "# max_story_length = max(story_length)\n",
    "# max_summary_length = max(summary_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max story length = 396 Max summary length = 128\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Max story length = {max_story_length}\", f\"Max summary length = {max_summary_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (312085, 500)\n"
     ]
    }
   ],
   "source": [
    "# Pad the datasets to have similar lengths\n",
    "story_data = pad_sequences(seq_story, maxlen=500)\n",
    "summary_data = pad_sequences(seq_summary, maxlen=500)\n",
    "\n",
    "print('Shape of data tensor:', story_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (312085, 500)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of label tensor:', summary_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(story_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "story_data = story_data[indices]\n",
    "summary_data = summary_data[indices]\n",
    "nb_validation_samples = int(0.20 * story_data.shape[0])\n",
    "\n",
    "x_train = story_data[:-nb_validation_samples]\n",
    "y_train = summary_data[:-nb_validation_samples]\n",
    "x_val = story_data[-nb_validation_samples:]\n",
    "y_val = summary_data[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dailymail Stories 2195884\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = {}\n",
    "f = open('/Users/dbm/Documents/Insight S19/data/glove.840B.300d.txt', encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "print('Loaded dailymail Stories %d' %len(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index_story) + 1, len(word_embeddings)))\n",
    "for word, i in word_idx.items():\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at the start of a big week for the higgs boson the most sought after particle in all of physics scientists in illinois said monday that they had crept closer to proving that the particle exists but had been unable to reach a definitive conclusion.',\n",
       " \"the scientists outlined their final analysis based on more than 10 years of research and 500 trillion particle collisions using the u.s. department of energy's fermilab tevatron collider near batavia illinois whose budgetary woes shut it down last year.\",\n",
       " 'what is the higgs boson and why is it important?',\n",
       " 'their announcement came two days before researchers at the large hadron collider under the alps are due to unveil their latest results at an eagerly awaited seminar at the cern particle physics laboratory in geneva switzerland.',\n",
       " 'our data strongly point toward the existence of the higgs boson rob roser a spokesman for one of two independent experiments at the tevatron said in a statement. but it will take results from the experiments at the large hadron collider in europe to establish a discovery.',\n",
       " 'read more the woman at the edge of physics',\n",
       " \"finding the higgs boson would help explain the origin of mass one of the open questions in physicists' current understanding of the way the universe works.\",\n",
       " \"the particle has been so difficult to pin down that the physicist leon lederman reportedly wanted to call his book the goddamn particle. but he truncated that epithet to the god particle which may have helped elevate the particle's allure in popular culture.\",\n",
       " 'more science news from cnn light years',\n",
       " 'the results from the tevatron stemming from the two different experiments suggest that if the higgs boson does exist it would have a mass between 115 and 135 gev about 130 times the mass of the proton.',\n",
       " 'before the tevatron closed the experiments there sent beams of particles whizzing around a four mile circumference in opposite directions. traveling at a fraction below the speed of light the particles would crash into each other creating conditions similar to those at the dawn of the universe for scientists to observe.',\n",
       " 'but so far neither the results from the u.s. collider experiments nor from the the large hadron collider located 328 feet underneath the border of france and switzerland have enough statistical significance to constitute a discovery.',\n",
       " \"it is easier to look for a friend's face in a sports stadium filled with 100 000 people than to search for a higgs like event among trillions of collisions said luciano ristori a physicist at the u.s. facility.\",\n",
       " \"attention now turns to the latest analysis of data from the 10 billion european machine the world's most powerful particle smasher.\",\n",
       " \"we now have more than double the data we had last year sergio bertolucci the director for research and computing at cern said last month. that should be enough to see whether the trends we were seeing in the 2011 data are still there or whether they've gone away. it's a very exciting time.\",\n",
       " \"scientists getting clearer picture of 'god particle'\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize_word(txt):\n",
    "    tokenized_words = []\n",
    "    # For each line in text\n",
    "    for l in txt:\n",
    "        # keep words\n",
    "        # Split by whitespace\n",
    "        tokenizer = RegexpTokenizer(r\"[a-z]+\")        \n",
    "        l = tokenizer.tokenize(l)\n",
    "        tokenized_words.append(l)\n",
    "    # Remove empty strings\n",
    "    tokenized_words = [c_txt for c_txt in tokenized_words if len(c_txt) > 0]\n",
    "    tokenized_words.append(\" \".join(l))\n",
    "    return tokenized_words\n",
    "\n",
    "story_w2v = []\n",
    "summary_w2v = []\n",
    "\n",
    "for i in news_summaries:\n",
    "    story_w2v.append(tokenize_word(i[\"story\"]))\n",
    "    summary_w2v.append(tokenize_word(i[\"summary\"]))\n",
    "# for i in story:\n",
    "#     tokenizer = RegexpTokenizer(r\"\\w+\")        \n",
    "#     i = tokenizer.tokenize(i)\n",
    "#     story_w2v.append(i)\n",
    "                     \n",
    "# for i in summary:    \n",
    "#     summary_w2v.append(gs.utils.simple_preprocess(i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['at',\n",
       "  'the',\n",
       "  'start',\n",
       "  'of',\n",
       "  'a',\n",
       "  'big',\n",
       "  'week',\n",
       "  'for',\n",
       "  'the',\n",
       "  'higgs',\n",
       "  'boson',\n",
       "  'the',\n",
       "  'most',\n",
       "  'sought',\n",
       "  'after',\n",
       "  'particle',\n",
       "  'in',\n",
       "  'all',\n",
       "  'of',\n",
       "  'physics',\n",
       "  'scientists',\n",
       "  'in',\n",
       "  'illinois',\n",
       "  'said',\n",
       "  'monday',\n",
       "  'that',\n",
       "  'they',\n",
       "  'had',\n",
       "  'crept',\n",
       "  'closer',\n",
       "  'to',\n",
       "  'proving',\n",
       "  'that',\n",
       "  'the',\n",
       "  'particle',\n",
       "  'exists',\n",
       "  'but',\n",
       "  'had',\n",
       "  'been',\n",
       "  'unable',\n",
       "  'to',\n",
       "  'reach',\n",
       "  'a',\n",
       "  'definitive',\n",
       "  'conclusion'],\n",
       " ['the',\n",
       "  'scientists',\n",
       "  'outlined',\n",
       "  'their',\n",
       "  'final',\n",
       "  'analysis',\n",
       "  'based',\n",
       "  'on',\n",
       "  'more',\n",
       "  'than',\n",
       "  'years',\n",
       "  'of',\n",
       "  'research',\n",
       "  'and',\n",
       "  'trillion',\n",
       "  'particle',\n",
       "  'collisions',\n",
       "  'using',\n",
       "  'the',\n",
       "  'u',\n",
       "  's',\n",
       "  'department',\n",
       "  'of',\n",
       "  'energy',\n",
       "  's',\n",
       "  'fermilab',\n",
       "  'tevatron',\n",
       "  'collider',\n",
       "  'near',\n",
       "  'batavia',\n",
       "  'illinois',\n",
       "  'whose',\n",
       "  'budgetary',\n",
       "  'woes',\n",
       "  'shut',\n",
       "  'it',\n",
       "  'down',\n",
       "  'last',\n",
       "  'year'],\n",
       " ['what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'higgs',\n",
       "  'boson',\n",
       "  'and',\n",
       "  'why',\n",
       "  'is',\n",
       "  'it',\n",
       "  'important'],\n",
       " ['their',\n",
       "  'announcement',\n",
       "  'came',\n",
       "  'two',\n",
       "  'days',\n",
       "  'before',\n",
       "  'researchers',\n",
       "  'at',\n",
       "  'the',\n",
       "  'large',\n",
       "  'hadron',\n",
       "  'collider',\n",
       "  'under',\n",
       "  'the',\n",
       "  'alps',\n",
       "  'are',\n",
       "  'due',\n",
       "  'to',\n",
       "  'unveil',\n",
       "  'their',\n",
       "  'latest',\n",
       "  'results',\n",
       "  'at',\n",
       "  'an',\n",
       "  'eagerly',\n",
       "  'awaited',\n",
       "  'seminar',\n",
       "  'at',\n",
       "  'the',\n",
       "  'cern',\n",
       "  'particle',\n",
       "  'physics',\n",
       "  'laboratory',\n",
       "  'in',\n",
       "  'geneva',\n",
       "  'switzerland'],\n",
       " ['our',\n",
       "  'data',\n",
       "  'strongly',\n",
       "  'point',\n",
       "  'toward',\n",
       "  'the',\n",
       "  'existence',\n",
       "  'of',\n",
       "  'the',\n",
       "  'higgs',\n",
       "  'boson',\n",
       "  'rob',\n",
       "  'roser',\n",
       "  'a',\n",
       "  'spokesman',\n",
       "  'for',\n",
       "  'one',\n",
       "  'of',\n",
       "  'two',\n",
       "  'independent',\n",
       "  'experiments',\n",
       "  'at',\n",
       "  'the',\n",
       "  'tevatron',\n",
       "  'said',\n",
       "  'in',\n",
       "  'a',\n",
       "  'statement',\n",
       "  'but',\n",
       "  'it',\n",
       "  'will',\n",
       "  'take',\n",
       "  'results',\n",
       "  'from',\n",
       "  'the',\n",
       "  'experiments',\n",
       "  'at',\n",
       "  'the',\n",
       "  'large',\n",
       "  'hadron',\n",
       "  'collider',\n",
       "  'in',\n",
       "  'europe',\n",
       "  'to',\n",
       "  'establish',\n",
       "  'a',\n",
       "  'discovery'],\n",
       " ['read', 'more', 'the', 'woman', 'at', 'the', 'edge', 'of', 'physics'],\n",
       " ['finding',\n",
       "  'the',\n",
       "  'higgs',\n",
       "  'boson',\n",
       "  'would',\n",
       "  'help',\n",
       "  'explain',\n",
       "  'the',\n",
       "  'origin',\n",
       "  'of',\n",
       "  'mass',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'open',\n",
       "  'questions',\n",
       "  'in',\n",
       "  'physicists',\n",
       "  'current',\n",
       "  'understanding',\n",
       "  'of',\n",
       "  'the',\n",
       "  'way',\n",
       "  'the',\n",
       "  'universe',\n",
       "  'works'],\n",
       " ['the',\n",
       "  'particle',\n",
       "  'has',\n",
       "  'been',\n",
       "  'so',\n",
       "  'difficult',\n",
       "  'to',\n",
       "  'pin',\n",
       "  'down',\n",
       "  'that',\n",
       "  'the',\n",
       "  'physicist',\n",
       "  'leon',\n",
       "  'lederman',\n",
       "  'reportedly',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'call',\n",
       "  'his',\n",
       "  'book',\n",
       "  'the',\n",
       "  'goddamn',\n",
       "  'particle',\n",
       "  'but',\n",
       "  'he',\n",
       "  'truncated',\n",
       "  'that',\n",
       "  'epithet',\n",
       "  'to',\n",
       "  'the',\n",
       "  'god',\n",
       "  'particle',\n",
       "  'which',\n",
       "  'may',\n",
       "  'have',\n",
       "  'helped',\n",
       "  'elevate',\n",
       "  'the',\n",
       "  'particle',\n",
       "  's',\n",
       "  'allure',\n",
       "  'in',\n",
       "  'popular',\n",
       "  'culture'],\n",
       " ['more', 'science', 'news', 'from', 'cnn', 'light', 'years'],\n",
       " ['the',\n",
       "  'results',\n",
       "  'from',\n",
       "  'the',\n",
       "  'tevatron',\n",
       "  'stemming',\n",
       "  'from',\n",
       "  'the',\n",
       "  'two',\n",
       "  'different',\n",
       "  'experiments',\n",
       "  'suggest',\n",
       "  'that',\n",
       "  'if',\n",
       "  'the',\n",
       "  'higgs',\n",
       "  'boson',\n",
       "  'does',\n",
       "  'exist',\n",
       "  'it',\n",
       "  'would',\n",
       "  'have',\n",
       "  'a',\n",
       "  'mass',\n",
       "  'between',\n",
       "  'and',\n",
       "  'gev',\n",
       "  'about',\n",
       "  'times',\n",
       "  'the',\n",
       "  'mass',\n",
       "  'of',\n",
       "  'the',\n",
       "  'proton'],\n",
       " ['before',\n",
       "  'the',\n",
       "  'tevatron',\n",
       "  'closed',\n",
       "  'the',\n",
       "  'experiments',\n",
       "  'there',\n",
       "  'sent',\n",
       "  'beams',\n",
       "  'of',\n",
       "  'particles',\n",
       "  'whizzing',\n",
       "  'around',\n",
       "  'a',\n",
       "  'four',\n",
       "  'mile',\n",
       "  'circumference',\n",
       "  'in',\n",
       "  'opposite',\n",
       "  'directions',\n",
       "  'traveling',\n",
       "  'at',\n",
       "  'a',\n",
       "  'fraction',\n",
       "  'below',\n",
       "  'the',\n",
       "  'speed',\n",
       "  'of',\n",
       "  'light',\n",
       "  'the',\n",
       "  'particles',\n",
       "  'would',\n",
       "  'crash',\n",
       "  'into',\n",
       "  'each',\n",
       "  'other',\n",
       "  'creating',\n",
       "  'conditions',\n",
       "  'similar',\n",
       "  'to',\n",
       "  'those',\n",
       "  'at',\n",
       "  'the',\n",
       "  'dawn',\n",
       "  'of',\n",
       "  'the',\n",
       "  'universe',\n",
       "  'for',\n",
       "  'scientists',\n",
       "  'to',\n",
       "  'observe'],\n",
       " ['but',\n",
       "  'so',\n",
       "  'far',\n",
       "  'neither',\n",
       "  'the',\n",
       "  'results',\n",
       "  'from',\n",
       "  'the',\n",
       "  'u',\n",
       "  's',\n",
       "  'collider',\n",
       "  'experiments',\n",
       "  'nor',\n",
       "  'from',\n",
       "  'the',\n",
       "  'the',\n",
       "  'large',\n",
       "  'hadron',\n",
       "  'collider',\n",
       "  'located',\n",
       "  'feet',\n",
       "  'underneath',\n",
       "  'the',\n",
       "  'border',\n",
       "  'of',\n",
       "  'france',\n",
       "  'and',\n",
       "  'switzerland',\n",
       "  'have',\n",
       "  'enough',\n",
       "  'statistical',\n",
       "  'significance',\n",
       "  'to',\n",
       "  'constitute',\n",
       "  'a',\n",
       "  'discovery'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'easier',\n",
       "  'to',\n",
       "  'look',\n",
       "  'for',\n",
       "  'a',\n",
       "  'friend',\n",
       "  's',\n",
       "  'face',\n",
       "  'in',\n",
       "  'a',\n",
       "  'sports',\n",
       "  'stadium',\n",
       "  'filled',\n",
       "  'with',\n",
       "  'people',\n",
       "  'than',\n",
       "  'to',\n",
       "  'search',\n",
       "  'for',\n",
       "  'a',\n",
       "  'higgs',\n",
       "  'like',\n",
       "  'event',\n",
       "  'among',\n",
       "  'trillions',\n",
       "  'of',\n",
       "  'collisions',\n",
       "  'said',\n",
       "  'luciano',\n",
       "  'ristori',\n",
       "  'a',\n",
       "  'physicist',\n",
       "  'at',\n",
       "  'the',\n",
       "  'u',\n",
       "  's',\n",
       "  'facility'],\n",
       " ['attention',\n",
       "  'now',\n",
       "  'turns',\n",
       "  'to',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'analysis',\n",
       "  'of',\n",
       "  'data',\n",
       "  'from',\n",
       "  'the',\n",
       "  'billion',\n",
       "  'european',\n",
       "  'machine',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'most',\n",
       "  'powerful',\n",
       "  'particle',\n",
       "  'smasher'],\n",
       " ['we',\n",
       "  'now',\n",
       "  'have',\n",
       "  'more',\n",
       "  'than',\n",
       "  'double',\n",
       "  'the',\n",
       "  'data',\n",
       "  'we',\n",
       "  'had',\n",
       "  'last',\n",
       "  'year',\n",
       "  'sergio',\n",
       "  'bertolucci',\n",
       "  'the',\n",
       "  'director',\n",
       "  'for',\n",
       "  'research',\n",
       "  'and',\n",
       "  'computing',\n",
       "  'at',\n",
       "  'cern',\n",
       "  'said',\n",
       "  'last',\n",
       "  'month',\n",
       "  'that',\n",
       "  'should',\n",
       "  'be',\n",
       "  'enough',\n",
       "  'to',\n",
       "  'see',\n",
       "  'whether',\n",
       "  'the',\n",
       "  'trends',\n",
       "  'we',\n",
       "  'were',\n",
       "  'seeing',\n",
       "  'in',\n",
       "  'the',\n",
       "  'data',\n",
       "  'are',\n",
       "  'still',\n",
       "  'there',\n",
       "  'or',\n",
       "  'whether',\n",
       "  'they',\n",
       "  've',\n",
       "  'gone',\n",
       "  'away',\n",
       "  'it',\n",
       "  's',\n",
       "  'a',\n",
       "  'very',\n",
       "  'exciting',\n",
       "  'time'],\n",
       " ['scientists', 'getting', 'clearer', 'picture', 'of', 'god', 'particle']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-270fb583f437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstory_w2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/insight_s19/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/insight_s19/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/insight_s19/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \"\"\"\n\u001b[1;32m    479\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 480\u001b[0;31m             sentences, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m/anaconda3/envs/insight_s19/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 )\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "story_w2v_model = gs.models.Word2Vec(story_w2v, size=350, window=10, min_count=2, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_w2v_model.train(story_w2v,total_examples=len(story_w2v),epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
