{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN+DailyMail Stories 312085\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "news_summaries = pickle.load(open('/Users/dbm/Documents/Insight S19/data/cnn_dmail_news_summary.pkl', 'rb'))\n",
    "print('Loaded CNN+DailyMail Stories %d' % len(news_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cleaning up text\n",
    "# def text_cleaner(txt):\n",
    "#     clean_txt = []\n",
    "#     # For each line in text\n",
    "#     for line in txt:\n",
    "#         # Remove newspaper info\n",
    "#         line = re.sub(pattern=\"[^a-zA-z]\", repl=\" \", string=line)\n",
    "#         line = line.lower()\n",
    "#         line = line.split()\n",
    "#         ps = PorterStemmer()\n",
    "#         line =[ps.stem(word) for word in line if not word in set(stopwords.words(\"english\"))]\n",
    "#         print(\"stem\", line)\n",
    "#         clean_txt.append(line) #\" \".join(line)\n",
    "# #         tokenizer = RegexpTokenizer(r\"[a-z]+\")\n",
    "# #         clean_txt = tokenizer.tokenize(clean_txt)\n",
    "#     # Remove empty strings\n",
    "#     clean_txt = [c_txt for c_txt in clean_txt if len(c_txt) > 0]\n",
    "#     return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cleaning up text\n",
    "def text_cleaner(line):\n",
    "    # For each line in text\n",
    "    # Remove newspaper info\n",
    "    line = re.sub(pattern=\"[^a-zA-z]\", repl=\" \", string=line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    ps = PorterStemmer()\n",
    "    line =[ps.stem(word) for word in line if not word in set(stopwords.words(\"english\"))]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_cleaner(txt):\n",
    "#     clean_txt = []\n",
    "#     # For each line in text\n",
    "#     for line in txt:\n",
    "#         # Remove newspaper info\n",
    "#         line = re.sub(\"\\\\(CNN\\\\)|\\\\(CNN\\\\) \\\\-\\\\- \", \"\", line)\n",
    "#         # Convert to lowercase\n",
    "#         line = line.lower()\n",
    "#         # Remove punctuation\n",
    "#         # Split by whitespace\n",
    "#         tokenizer = RegexpTokenizer(r\"[a-z]+\")\n",
    "#         # stem words\n",
    "#         ps = PorterStemmer()\n",
    "#         line =[ps.stem(word) for word in line if not word in set(stopwords.words(\"english\"))]\n",
    "#         clean_txt.append(\" \".join(line))\n",
    "#     # Remove empty strings\n",
    "#     clean_txt = [c_txt for c_txt in clean_txt if len(c_txt) > 0]\n",
    "#     return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem ['start', 'big', 'week', 'higg', 'boson', 'sought', 'particl', 'physic', 'scientist', 'illinoi', 'said', 'monday', 'crept', 'closer', 'prove', 'particl', 'exist', 'unabl', 'reach', 'definit', 'conclus', 'scientist', 'outlin', 'final', 'analysi', 'base', 'year', 'research', 'trillion', 'particl', 'collis', 'use', 'u', 'depart', 'energi', 'fermilab', 'tevatron', 'collid', 'near', 'batavia', 'illinoi', 'whose', 'budgetari', 'woe', 'shut', 'last', 'year', 'higg', 'boson', 'import', 'announc', 'came', 'two', 'day', 'research', 'larg', 'hadron', 'collid', 'alp', 'due', 'unveil', 'latest', 'result', 'eagerli', 'await', 'seminar', 'cern', 'particl', 'physic', 'laboratori', 'geneva', 'switzerland', 'data', 'strongli', 'point', 'toward', 'exist', 'higg', 'boson', 'rob', 'roser', 'spokesman', 'one', 'two', 'independ', 'experi', 'tevatron', 'said', 'statement', 'take', 'result', 'experi', 'larg', 'hadron', 'collid', 'europ', 'establish', 'discoveri', 'read', 'woman', 'edg', 'physic', 'find', 'higg', 'boson', 'would', 'help', 'explain', 'origin', 'mass', 'one', 'open', 'question', 'physicist', 'current', 'understand', 'way', 'univers', 'work', 'particl', 'difficult', 'pin', 'physicist', 'leon', 'lederman', 'reportedli', 'want', 'call', 'book', 'goddamn', 'particl', 'truncat', 'epithet', 'god', 'particl', 'may', 'help', 'elev', 'particl', 'allur', 'popular', 'cultur', 'scienc', 'news', 'cnn', 'light', 'year', 'result', 'tevatron', 'stem', 'two', 'differ', 'experi', 'suggest', 'higg', 'boson', 'exist', 'would', 'mass', 'gev', 'time', 'mass', 'proton', 'tevatron', 'close', 'experi', 'sent', 'beam', 'particl', 'whizz', 'around', 'four', 'mile', 'circumfer', 'opposit', 'direct', 'travel', 'fraction', 'speed', 'light', 'particl', 'would', 'crash', 'creat', 'condit', 'similar', 'dawn', 'univers', 'scientist', 'observ', 'far', 'neither', 'result', 'u', 'collid', 'experi', 'larg', 'hadron', 'collid', 'locat', 'feet', 'underneath', 'border', 'franc', 'switzerland', 'enough', 'statist', 'signific', 'constitut', 'discoveri', 'easier', 'look', 'friend', 'face', 'sport', 'stadium', 'fill', 'peopl', 'search', 'higg', 'like', 'event', 'among', 'trillion', 'collis', 'said', 'luciano', 'ristori', 'physicist', 'u', 'facil', 'attent', 'turn', 'latest', 'analysi', 'data', 'billion', 'european', 'machin', 'world', 'power', 'particl', 'smasher', 'doubl', 'data', 'last', 'year', 'sergio', 'bertolucci', 'director', 'research', 'comput', 'cern', 'said', 'last', 'month', 'enough', 'see', 'whether', 'trend', 'see', 'data', 'still', 'whether', 'gone', 'away', 'excit', 'time', 'scientist', 'get', 'clearer', 'pictur', 'god', 'particl']\n"
     ]
    }
   ],
   "source": [
    "# line = \" \". join(news_summaries[0][\"story\"])\n",
    "\n",
    "# # text_cleaner([story])\n",
    "# line = re.sub(pattern=\"[^a-zA-z]\", repl=\" \", string=line)\n",
    "# line = line.lower()\n",
    "# line = line.split()\n",
    "# ps = PorterStemmer()\n",
    "# line =[ps.stem(word) for word in line if not word in set(stopwords.words(\"english\"))]\n",
    "# print(\"stem\", line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8e6a98a2364a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# story = re.sub(\"\\\\(CNN\\\\)|\\\\(CNN\\\\) \\\\-\\\\- \", \"\", story)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# # Remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "# story = re.sub(\"\\\\(CNN\\\\)|\\\\(CNN\\\\) \\\\-\\\\- \", \"\", story)\n",
    "# Convert to lowercase\n",
    "story = story.lower()\n",
    "print(story)\n",
    "# # Remove punctuation\n",
    "# # Split by whitespace\n",
    "# tokenizer = RegexpTokenizer(r\"[a-z]+\")\n",
    "# # stem words\n",
    "# ps = PorterStemmer()\n",
    "# story =[ps.stem(word) for word in line if not word in set(stopwords.words(\"english\"))]\n",
    "# clean_txt.append(\" \".join(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = []\n",
    "summary = []\n",
    "# Clean stories\n",
    "for line in news_summaries[:1]:\n",
    "    story.append(text_cleaner(\" \".join(line[\"story\"]))) \n",
    "    summary.append(text_cleaner(\" \".join(line[\"summary\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'base',\n",
       " 'scientist',\n",
       " 'say',\n",
       " 'data',\n",
       " 'point',\n",
       " 'toward',\n",
       " 'exist',\n",
       " 'higg',\n",
       " 'boson',\n",
       " 'find',\n",
       " 'higg',\n",
       " 'boson',\n",
       " 'would',\n",
       " 'help',\n",
       " 'explain',\n",
       " 'origin',\n",
       " 'mass',\n",
       " 'research',\n",
       " 'tevatron',\n",
       " 'collid',\n",
       " 'provid',\n",
       " 'conclus',\n",
       " 'answer',\n",
       " 'attent',\n",
       " 'turn',\n",
       " 'seminar',\n",
       " 'wednesday',\n",
       " 'data',\n",
       " 'larg',\n",
       " 'hadron',\n",
       " 'collid']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package for tokenizing text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc,\n",
    "                     lowercase=False, \n",
    "                     max_features=1500)\n",
    "# Create a sparse matrix: matrix of features including the IVs\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
